{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNoCqM1I5-le"
   },
   "source": [
    "# Training Pipeline\n",
    "\n",
    "Training the best M models\n",
    "\n",
    "Incluye:\n",
    "\n",
    "- Tuning de hyperparámetros (con meses históricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#######\n",
    "# rutas\n",
    "# datasets\n",
    "from config import dataset_file_fe6_6pqt\n",
    "# optimizacion\n",
    "from config import db_path\n",
    "# modelos\n",
    "from config import modelos_path\n",
    "# predicciones\n",
    "from config import pred_path\n",
    "\n",
    "##########\n",
    "# pipeline\n",
    "from processing import ModelPipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganancia_acierto = 273000\n",
    "costo_estimulo = 7000\n",
    "\n",
    "mes_train_all = [201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908,\n",
    "                 201909, 201910, 201911, 201912, 202001, 202002, 202003, 202004,\n",
    "                 202005, 202006, 202007, 202008, 202009, 202010, 202011, 202012,\n",
    "                 202101, 202102, 202103, 202104, 202105, 202106]\n",
    "\n",
    "mes_train_ult_3_meses = [202104, 202105, 202106]\n",
    "\n",
    "mes_train_ult_6_meses = [202101, 202102, 202103, 202104, 202105, 202106]\n",
    "\n",
    "mes_train_ult_9_meses = [202009, 202010, 202011, 202012, 202101, 202102, 202103, 202104, 202105, 202106]\n",
    "\n",
    "mes_train_ult_anio = [202006, 202007, 202008, 202009, 202010, 202011, 202012,\n",
    "                 202101, 202102, 202103, 202104, 202105, 202106]\n",
    "\n",
    "mes_train = [202106]\n",
    "mes_test = 202108\n",
    "\n",
    "threshold = 0.025\n",
    "\n",
    "semillas = [437809, 327347, 392879, 455783, 217163]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM \n",
    "\n",
    "**Prepro in 6 months and Conceptual FE 6 months**\n",
    "\n",
    "> comp02_prepro_6.ipynb\n",
    "\n",
    "> comp02_fe6_6.ipynb\n",
    "\n",
    "**Usando los últimos 3 meses para optimizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(dataset_file_fe6_6pqt)\n",
    "\n",
    "# running local\n",
    "data = pd.read_parquet(\"datos/datasets_competencia_02_fe6_6_3m_train.parquet\")\n",
    "\n",
    "# Mapear etiquetas de clase a números\n",
    "label_mapping = {'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2}\n",
    "\n",
    "data['clase_ternaria'] = data['clase_ternaria'].map(label_mapping)\n",
    "\n",
    "X_train = data[data['foto_mes'].isin(mes_train_ult_3_meses)]\n",
    "y_train = X_train['clase_ternaria']\n",
    "X_train = X_train.drop(columns=['clase_ternaria'])\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corriendo pipeline con LightGBM ###\n",
      "Columns with all NaN values: ['payroll_slope_1_foto_mes', 'cuenta_corriente_slope_1_foto_mes', 'visa_consumo_slope_1_foto_mes', 'comisiones_mantenimiento_slope_1_foto_mes', 'comisiones_otras_slope_1_foto_mes']\n",
      "\n",
      "# Optimizando el modelo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-20 07:56:19,158] Using an existing study with name 'exp_lgbm_pr6_fe6_tr3_x' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizando lightgbm con 25 pruebas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-20 07:59:34,886] Trial 176 finished with value: 347522000.0 and parameters: {'n_estimators': 457, 'num_leaves': 54, 'learning_rate': 0.0270460231088841, 'min_data_in_leaf': 97, 'min_gain_to_split': 0.02583464978259349, 'feature_fraction': 0.5069688756627543, 'bagging_fraction': 0.8477537070962985, 'bagging_freq': 1, 'max_bin': 101}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:02:50,430] Trial 177 finished with value: 349365333.3333334 and parameters: {'n_estimators': 465, 'num_leaves': 50, 'learning_rate': 0.022038529551349276, 'min_data_in_leaf': 96, 'min_gain_to_split': 0.046378583546193494, 'feature_fraction': 0.5181987875516293, 'bagging_fraction': 0.8495833943361905, 'bagging_freq': 2, 'max_bin': 92}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:06:08,391] Trial 178 finished with value: 351409333.3333334 and parameters: {'n_estimators': 452, 'num_leaves': 56, 'learning_rate': 0.02694228790144789, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.00015889971686527428, 'feature_fraction': 0.5023783247459099, 'bagging_fraction': 0.8414186748652362, 'bagging_freq': 2, 'max_bin': 99}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:09:46,792] Trial 179 finished with value: 350317333.3333334 and parameters: {'n_estimators': 449, 'num_leaves': 57, 'learning_rate': 0.022773307704138608, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.07537102141268226, 'feature_fraction': 0.5307503622669358, 'bagging_fraction': 0.836766063890288, 'bagging_freq': 2, 'max_bin': 98}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:12:35,569] Trial 180 finished with value: 345986666.6666667 and parameters: {'n_estimators': 435, 'num_leaves': 58, 'learning_rate': 0.02215125358063861, 'min_data_in_leaf': 95, 'min_gain_to_split': 0.07223976611863824, 'feature_fraction': 0.5213088406249055, 'bagging_fraction': 0.837671938239439, 'bagging_freq': 2, 'max_bin': 88}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:15:58,302] Trial 181 finished with value: 350140000.00000006 and parameters: {'n_estimators': 442, 'num_leaves': 64, 'learning_rate': 0.02837581208903769, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.0010078702921271585, 'feature_fraction': 0.5369909117927747, 'bagging_fraction': 0.8298353489345931, 'bagging_freq': 2, 'max_bin': 100}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:19:33,108] Trial 182 finished with value: 344596000.00000006 and parameters: {'n_estimators': 461, 'num_leaves': 64, 'learning_rate': 0.029238719228880352, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.04820272412643389, 'feature_fraction': 0.5378695081845545, 'bagging_fraction': 0.8303148025675849, 'bagging_freq': 2, 'max_bin': 109}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:22:39,550] Trial 183 finished with value: 348287333.3333334 and parameters: {'n_estimators': 441, 'num_leaves': 59, 'learning_rate': 0.023661105366260805, 'min_data_in_leaf': 93, 'min_gain_to_split': 0.13057254335306723, 'feature_fraction': 0.5141999717384362, 'bagging_fraction': 0.8576624332109634, 'bagging_freq': 1, 'max_bin': 82}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:25:53,767] Trial 184 finished with value: 352053333.3333334 and parameters: {'n_estimators': 464, 'num_leaves': 49, 'learning_rate': 0.03531171245960747, 'min_data_in_leaf': 96, 'min_gain_to_split': 0.00021008890819528255, 'feature_fraction': 0.5284053583437764, 'bagging_fraction': 0.8381351208125363, 'bagging_freq': 2, 'max_bin': 99}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:29:04,205] Trial 185 finished with value: 349804000.0 and parameters: {'n_estimators': 467, 'num_leaves': 45, 'learning_rate': 0.03503140156682807, 'min_data_in_leaf': 96, 'min_gain_to_split': 0.006682098109645755, 'feature_fraction': 0.5284345223730256, 'bagging_fraction': 0.8221809462514228, 'bagging_freq': 2, 'max_bin': 98}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:32:52,581] Trial 186 finished with value: 348385333.3333334 and parameters: {'n_estimators': 447, 'num_leaves': 66, 'learning_rate': 0.030000709929130096, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.00177732320506354, 'feature_fraction': 0.5388623856872367, 'bagging_fraction': 0.838857687549717, 'bagging_freq': 2, 'max_bin': 92}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:36:11,597] Trial 187 finished with value: 345856000.0 and parameters: {'n_estimators': 468, 'num_leaves': 49, 'learning_rate': 0.037479750683743845, 'min_data_in_leaf': 95, 'min_gain_to_split': 0.029282967052828554, 'feature_fraction': 0.522710061858798, 'bagging_fraction': 0.830018088926955, 'bagging_freq': 2, 'max_bin': 99}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:39:26,880] Trial 188 finished with value: 351040666.66666675 and parameters: {'n_estimators': 439, 'num_leaves': 55, 'learning_rate': 0.02864558254285227, 'min_data_in_leaf': 99, 'min_gain_to_split': 0.07514531814755002, 'feature_fraction': 0.5145760858826411, 'bagging_fraction': 0.8559523400481853, 'bagging_freq': 2, 'max_bin': 109}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:42:55,324] Trial 189 finished with value: 350630000.0 and parameters: {'n_estimators': 428, 'num_leaves': 57, 'learning_rate': 0.028563745121097977, 'min_data_in_leaf': 99, 'min_gain_to_split': 0.07368149454262438, 'feature_fraction': 0.507898748543811, 'bagging_fraction': 0.8545890402734921, 'bagging_freq': 2, 'max_bin': 109}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:46:26,998] Trial 190 finished with value: 347176666.6666667 and parameters: {'n_estimators': 425, 'num_leaves': 57, 'learning_rate': 0.03320562193955309, 'min_data_in_leaf': 99, 'min_gain_to_split': 0.07083962000891014, 'feature_fraction': 0.503570408641653, 'bagging_fraction': 0.8565817689721953, 'bagging_freq': 2, 'max_bin': 110}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:49:51,007] Trial 191 finished with value: 349766666.6666667 and parameters: {'n_estimators': 416, 'num_leaves': 45, 'learning_rate': 0.025561720664728883, 'min_data_in_leaf': 92, 'min_gain_to_split': 0.05041358690375219, 'feature_fraction': 0.5117437217204316, 'bagging_fraction': 0.8427987729354948, 'bagging_freq': 2, 'max_bin': 107}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:52:34,950] Trial 192 finished with value: 346014666.6666667 and parameters: {'n_estimators': 438, 'num_leaves': 64, 'learning_rate': 0.030008724248121583, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.027372704744784267, 'feature_fraction': 0.5180997006082833, 'bagging_fraction': 0.8532889717496225, 'bagging_freq': 2, 'max_bin': 102}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 08:57:40,443] Trial 193 finished with value: 285576666.6666667 and parameters: {'n_estimators': 431, 'num_leaves': 225, 'learning_rate': 0.027624313742128617, 'min_data_in_leaf': 100, 'min_gain_to_split': 0.07477966251894409, 'feature_fraction': 0.5271052786452215, 'bagging_fraction': 0.8622625800317065, 'bagging_freq': 2, 'max_bin': 114}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:00:34,408] Trial 194 finished with value: 347643333.3333334 and parameters: {'n_estimators': 452, 'num_leaves': 60, 'learning_rate': 0.03491190491876396, 'min_data_in_leaf': 96, 'min_gain_to_split': 0.001789620262984619, 'feature_fraction': 0.511491179133137, 'bagging_fraction': 0.8239341708618827, 'bagging_freq': 2, 'max_bin': 99}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:03:51,874] Trial 195 finished with value: 351488666.6666667 and parameters: {'n_estimators': 444, 'num_leaves': 56, 'learning_rate': 0.024001299181692318, 'min_data_in_leaf': 98, 'min_gain_to_split': 0.048431746941861094, 'feature_fraction': 0.5346969714810007, 'bagging_fraction': 0.8346961262058885, 'bagging_freq': 2, 'max_bin': 95}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:07:27,523] Trial 196 finished with value: 349701333.3333334 and parameters: {'n_estimators': 457, 'num_leaves': 55, 'learning_rate': 0.022862379670201452, 'min_data_in_leaf': 94, 'min_gain_to_split': 0.0495655281178255, 'feature_fraction': 0.5003229570083716, 'bagging_fraction': 0.8440210824356458, 'bagging_freq': 2, 'max_bin': 92}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:10:41,898] Trial 197 finished with value: 347340000.0 and parameters: {'n_estimators': 448, 'num_leaves': 51, 'learning_rate': 0.024783327004160492, 'min_data_in_leaf': 96, 'min_gain_to_split': 0.08067875305464883, 'feature_fraction': 0.5289025590360892, 'bagging_fraction': 0.837654167081546, 'bagging_freq': 1, 'max_bin': 110}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:13:46,759] Trial 198 finished with value: 342734000.00000006 and parameters: {'n_estimators': 464, 'num_leaves': 56, 'learning_rate': 0.042904420357360605, 'min_data_in_leaf': 99, 'min_gain_to_split': 0.03577321997811133, 'feature_fraction': 0.518172445892203, 'bagging_fraction': 0.8546329836994323, 'bagging_freq': 2, 'max_bin': 95}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:17:27,302] Trial 199 finished with value: 347424000.0 and parameters: {'n_estimators': 429, 'num_leaves': 47, 'learning_rate': 0.021496915313766626, 'min_data_in_leaf': 97, 'min_gain_to_split': 0.10625809398057642, 'feature_fraction': 0.5361611722069711, 'bagging_fraction': 0.8194272640095644, 'bagging_freq': 2, 'max_bin': 117}. Best is trial 169 with value: 352865333.3333334.\n",
      "[I 2024-11-20 09:20:20,970] Trial 200 finished with value: 348390000.0 and parameters: {'n_estimators': 437, 'num_leaves': 60, 'learning_rate': 0.031576443776670335, 'min_data_in_leaf': 100, 'min_gain_to_split': 0.058891561712398954, 'feature_fraction': 0.5246460996791911, 'bagging_fraction': 0.8481008601664841, 'bagging_freq': 2, 'max_bin': 107}. Best is trial 169 with value: 352865333.3333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros para lightgbm: {'n_estimators': 466, 'num_leaves': 57, 'learning_rate': 0.024563319625208798, 'min_data_in_leaf': 100, 'min_gain_to_split': 0.011030284732349089, 'feature_fraction': 0.5157056981996073, 'bagging_fraction': 0.8424259086760796, 'bagging_freq': 2, 'max_bin': 97}\n"
     ]
    }
   ],
   "source": [
    "# Condiciones de la optimización\n",
    "s = 1\n",
    "prepro = 6 # data quality + data drifting\n",
    "fe = 6 # feature engineering conceptual 6 meses\n",
    "training = 3 # un mes de optimización\n",
    "\n",
    "print(\"### Corriendo pipeline con LightGBM ###\")\n",
    "# Inicializar el pipeline con 'lightgbm'\n",
    "pipeline_lgbm = ModelPipeline(data=None, seeds=semillas,\n",
    "                              model_type='lightgbm', seed=s, n_jobs=-1)\n",
    "\n",
    "# performed manually to reduce memory\n",
    "# X_train, y_train = pipeline_lgbm.def_xy(mes_train_ult_3_meses)\n",
    "\n",
    "# Identify columns with all NaN values\n",
    "cols_with_all_nan = X_train.columns[X_train.isna().all()]\n",
    "print(\"Columns with all NaN values:\", cols_with_all_nan.tolist())\n",
    "\n",
    "# Drop these columns\n",
    "X_train = X_train.drop(columns=cols_with_all_nan) # extra limpieza\n",
    "\n",
    "# Imputación de valores faltantes\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "X_train_imp = pd.DataFrame(imp_median.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "del X_train\n",
    "\n",
    "# Opcional: Codificar variables categóricas\n",
    "# LightGBM puede manejar variables categóricas directamente si se especifican\n",
    "# Si tus datos tienen variables categóricas, puedes identificarlas y especificarlas en el modelo\n",
    "categorical_features = [col for col in X_train_imp.columns if X_train_imp[col].dtype == 'object']\n",
    "\n",
    "# Convertir variables categóricas a 'category' dtype para LightGBM\n",
    "for col in categorical_features:\n",
    "    X_train_imp[col] = X_train_imp[col].astype('category')\n",
    "\n",
    "# Definir el almacenamiento para Optuna\n",
    "# storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
    "storage_name = \"sqlite:///optimizacion/optimization_tree.db\"\n",
    "study_name = f\"exp_lgbm_pr{prepro}_fe{fe}_tr{training}_x\"\n",
    "\n",
    "print(\"\\n# Optimizando el modelo\")\n",
    "pipeline_lgbm.optimize_model(\n",
    "    X_train_imp, y_train,\n",
    "    storage_name=storage_name,\n",
    "    study_name=study_name,\n",
    "    optimize=False,\n",
    "    n_trials=200\n",
    ")\n",
    "\n",
    "del X_train_imp\n",
    "del y_train\n",
    "del pipeline_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost \n",
    "\n",
    "**Prepro in 6 months and Conceptual FE 6 months**\n",
    "\n",
    "> comp02_prepro_6.ipynb\n",
    "\n",
    "> comp02_fe6_6.ipynb\n",
    "\n",
    "**Usando los últimos 3 meses para optimizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_parquet(dataset_file_fe6_6pqt)\n",
    "# running local\n",
    "data = pd.read_parquet(\"datos/datasets_competencia_02_fe6_6_3m_train.parquet\")\n",
    "\n",
    "# Mapear etiquetas de clase a números\n",
    "label_mapping = {'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2}\n",
    "\n",
    "data['clase_ternaria'] = data['clase_ternaria'].map(label_mapping)\n",
    "\n",
    "X_train = data[data['foto_mes'].isin(mes_train_ult_3_meses)]\n",
    "y_train = X_train['clase_ternaria']\n",
    "X_train = X_train.drop(columns=['clase_ternaria'])\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corriendo pipeline con XGBoost ###\n",
      "Columns with all NaN values: ['payroll_slope_1_foto_mes', 'cuenta_corriente_slope_1_foto_mes', 'visa_consumo_slope_1_foto_mes', 'comisiones_mantenimiento_slope_1_foto_mes', 'comisiones_otras_slope_1_foto_mes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 07:56:35,272] Using an existing study with name 'exp_xgb_pr6_fe6_tr3_x' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Optimizando el modelo\n",
      "Mejores parámetros para xgboost: {'n_estimators': 464, 'max_leaves': 228, 'eta': 0.026841741174110256, 'gamma': 0.6065611085207565, 'min_child_weight': 10, 'subsample': 0.8649413237261332, 'colsample_bytree': 0.5013152719066779}\n"
     ]
    }
   ],
   "source": [
    "# Condiciones de la optimización\n",
    "s = 1\n",
    "prepro = 6 # data quality + data drifting\n",
    "fe = 6 # feature engineering conceptual 6 meses\n",
    "training = 3 # un mes de optimización\n",
    "\n",
    "print(\"### Corriendo pipeline con XGBoost ###\")\n",
    "# Inicializar el pipeline con 'xgboost'\n",
    "pipeline_xgb = ModelPipeline(data=None, seeds=semillas, \n",
    "                              model_type='xgboost', seed=s, n_jobs=-1)\n",
    "\n",
    "# performed manually to reduce memory\n",
    "# X_train, y_train = pipeline_xgb.def_xy(mes_train_ult_3_meses)\n",
    "\n",
    "# Identify columns with all NaN values\n",
    "cols_with_all_nan = X_train.columns[X_train.isna().all()]\n",
    "print(\"Columns with all NaN values:\", cols_with_all_nan.tolist())\n",
    "\n",
    "# Drop these columns\n",
    "X_train = X_train.drop(columns=cols_with_all_nan) # extra limpieza\n",
    "\n",
    "# Imputación de valores faltantes\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "X_train_imp = pd.DataFrame(imp_median.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "del X_train\n",
    "\n",
    "# Codificar variables categóricas\n",
    "categorical_features = [col for col in X_train_imp.columns if X_train_imp[col].dtype == 'object']\n",
    "\n",
    "# Convertir variables categóricas a 'category' dtype para LightGBM\n",
    "for col in categorical_features:\n",
    "    X_train_imp[col] = X_train_imp[col].astype('category')\n",
    "\n",
    "# Definir el almacenamiento para Optuna\n",
    "# storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm.db\"\n",
    "storage_name = \"sqlite:///optimizacion/optimization_tree.db\"\n",
    "study_name = f\"exp_xgb_pr{prepro}_fe{fe}_tr{training}_x\"\n",
    "\n",
    "print(\"\\n# Optimizando el modelo\")\n",
    "pipeline_xgb.optimize_model(\n",
    "    X_train_imp, y_train,\n",
    "    storage_name=storage_name,\n",
    "    study_name=study_name,\n",
    "    optimize=False,  \n",
    "    n_trials=200\n",
    ")\n",
    "\n",
    "del X_train_imp\n",
    "del y_train\n",
    "del pipeline_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dm_eyf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
