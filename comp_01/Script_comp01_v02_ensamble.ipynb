{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNoCqM1I5-le"
      },
      "source": [
        "# Scrip para la competencia 01\n",
        "\n",
        "Training the best M models\n",
        "\n",
        "Incluye:\n",
        "\n",
        "- Tuning de hyperparámetros (con meses históricos)\n",
        "- Análisis de la incertidumbre (respecto a la proyección histórica)\n",
        "- Predicción para entrega (competancia de Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Directorios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "datos = 'datos'\n",
        "\n",
        "optimizacion = 'optimizacion'\n",
        "\n",
        "modelos = 'modelos'\n",
        "\n",
        "resultados = 'resultados'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Condiciones de entrenamiento\n",
        "\n",
        "Entrenamos con la mejor optimización posible, sobre el último més disponible.\n",
        "\n",
        "Ya tenemos en nuestras variables, algunas variables históricas según el feature engineering aplicado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "\n",
        "mes_train = 202104\n",
        "\n",
        "threshold = 0.025\n",
        "\n",
        "semillas = [437809, 327347, 392879, 455783, 217163]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_jobs = os.cpu_count() - 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline\n",
        "\n",
        "Con distintos modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar las librerías necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
        "from joblib import Parallel, delayed\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "\n",
        "# Definir la clase ModelPipeline\n",
        "class ModelPipeline:\n",
        "    def __init__(self, data, seeds, model_type='decision_tree', \n",
        "                 ganancia_acierto=273000, costo_estimulo=7000, \n",
        "                 threshold=0.025, seed=0, n_jobs=-1, reg=False):\n",
        "        self.data = data\n",
        "        self.seeds = seeds\n",
        "        self.s = seed\n",
        "        self.n_jobs = int(n_jobs)\n",
        "        self.model_type = model_type\n",
        "        self.ganancia_acierto = ganancia_acierto\n",
        "        self.costo_estimulo = costo_estimulo\n",
        "        self.threshold = threshold\n",
        "        self.reg = reg\n",
        "        self.models = {}\n",
        "        self.base_params = {'random_state': self.seeds[self.s]}\n",
        "        self.best_params = None\n",
        "        self.base_model = None\n",
        "        self.best_model = None\n",
        "\n",
        "        # Mapear model_type al clasificador correspondiente\n",
        "        self.classifier_map = {\n",
        "            'decision_tree': DecisionTreeClassifier,\n",
        "            'random_forest': RandomForestClassifier,\n",
        "            'xgboost': XGBClassifier,\n",
        "            'lightgbm': LGBMClassifier\n",
        "        }\n",
        "\n",
        "    def def_xy(self, mes, target='clase_ternaria', to_pred=False):\n",
        "        X = self.data[self.data['foto_mes'] == mes]\n",
        "        y = X[target]\n",
        "        X = X.drop(columns=[target])\n",
        "\n",
        "        numero_de_cliente = X['numero_de_cliente']\n",
        "\n",
        "        if to_pred:\n",
        "            return X, numero_de_cliente\n",
        "        else:\n",
        "            return X, y\n",
        "\n",
        "    def ganancia(self, model, X, y, prop=1):\n",
        "        # Obtener las probabilidades predichas\n",
        "        y_hat = model.predict_proba(X)\n",
        "\n",
        "        # Obtener las clases del modelo\n",
        "        model_classes = model.classes_\n",
        "\n",
        "        # Identificar la clase objetivo (puede ser 'BAJA+2' o 2)\n",
        "        if 'BAJA+2' in model_classes:\n",
        "            target_class = 'BAJA+2'\n",
        "        elif 2 in model_classes:\n",
        "            target_class = 2\n",
        "        else:\n",
        "            raise ValueError(\"La clase objetivo 'BAJA+2' o 2 no está en las clases del modelo.\")\n",
        "\n",
        "        # Obtener el índice de la clase objetivo\n",
        "        class_index = np.where(model_classes == target_class)[0][0]\n",
        "\n",
        "        # Obtener las probabilidades predichas para la clase objetivo\n",
        "        probs = y_hat[:, class_index]\n",
        "\n",
        "        # Calcular la ganancia para cada fila\n",
        "        gains = np.where(\n",
        "            probs >= self.threshold,\n",
        "            np.where(y == target_class, self.ganancia_acierto, -self.costo_estimulo),\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Sumar las ganancias\n",
        "        total_gain = gains.sum()/prop\n",
        "\n",
        "        return total_gain\n",
        "\n",
        "    def train_and_evaluate(self, train_index, test_index, X, y, params):\n",
        "        # Instanciar el clasificador basado en model_type\n",
        "        classifier_class = self.classifier_map[self.model_type]\n",
        "        model = classifier_class(**params)\n",
        "        model.fit(X.iloc[train_index], y.iloc[train_index])\n",
        "        ganancia_value = self.ganancia(model, X.iloc[test_index], y.iloc[test_index], prop=0.3)\n",
        "        return model, ganancia_value\n",
        "\n",
        "    def optimize_model(self, X, y, storage_name, study_name, optimize=True, n_trials=200):\n",
        "        sss_opt = ShuffleSplit(n_splits=5, test_size=0.3, random_state=self.seeds[self.s])\n",
        "\n",
        "        def objective_xgboost(trial):\n",
        "            # Hiperparámetros para XGBClassifier\n",
        "\n",
        "            # Parámetros a optimizar\n",
        "            # n_estimators = trial.suggest_int('n_estimators', 100, 500)\n",
        "            max_leaves = trial.suggest_int('max_leaves', 10, 256)\n",
        "            learning_rate = trial.suggest_float('eta', 0.01, 0.3, log=True)  # 'eta' es equivalente a 'learning_rate'\n",
        "            gamma = trial.suggest_float('gamma', 0, 5)\n",
        "            min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
        "            subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
        "            colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
        "            if self.reg:\n",
        "                reg_lambda = trial.suggest_float('lambda', 0.0, 10.0)\n",
        "                reg_alpha = trial.suggest_float('alpha', 0.0, 10.0)\n",
        "            # scale_pos_weight = trial.suggest_float('scale_pos_weight', 1.0, 10.0)\n",
        "\n",
        "            params = {\n",
        "                'booster': 'gbtree',\n",
        "                'n_estimators': 100,\n",
        "                'max_leaves': max_leaves,\n",
        "                'learning_rate': learning_rate,\n",
        "                'gamma': gamma,\n",
        "                'min_child_weight': min_child_weight,\n",
        "                'subsample': subsample,\n",
        "                'colsample_bytree': colsample_bytree,\n",
        "                # 'scale_pos_weight': scale_pos_weight, # default = 1, ya que la ganancia ya contempla desbalance\n",
        "                'random_state': self.seeds[self.s],\n",
        "                'enable_categorical': True,\n",
        "                'use_label_encoder': False,\n",
        "                'objective': 'multi:softprob',\n",
        "                'num_class': 3,\n",
        "                'eval_metric': 'mlogloss',\n",
        "                'tree_method': 'hist',      # Usar 'hist' para grandes conjuntos de datos\n",
        "                'grow_policy': 'lossguide', # Necesario cuando se usa 'max_leaves'\n",
        "            }\n",
        "\n",
        "            if self.reg:\n",
        "                params.update({\n",
        "                    'reg_lambda': reg_lambda,  # 'lambda' es palabra reservada en Python, usamos 'reg_lambda'\n",
        "                    'reg_alpha': reg_alpha,\n",
        "                })\n",
        "\n",
        "            # Ejecutar validación cruzada paralela\n",
        "            results = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(self.train_and_evaluate)(train_index, test_index, X, y, params)\n",
        "                for train_index, test_index in sss_opt.split(X, y)\n",
        "            )\n",
        "\n",
        "            # Retornar la ganancia media\n",
        "            return np.mean([result[1] for result in results])\n",
        "\n",
        "        def objective_lightgbm(trial):\n",
        "            # Hiperparámetros para LGBMClassifier\n",
        "            # n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
        "            num_leaves = trial.suggest_int('num_leaves', 31, 256)\n",
        "            # max_depth = trial.suggest_int('max_depth', -1, 30) # conflict with num_leaves\n",
        "            learning_rate = trial.suggest_float('learning_rate', 0.001, 0.3, log=True)\n",
        "            min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 20, 100)\n",
        "            if self.reg:\n",
        "                lambda_l1 = trial.suggest_float('lambda_l1', 0.0, 10.0)\n",
        "                lambda_l2 = trial.suggest_float('lambda_l2', 0.0, 10.0)\n",
        "            min_gain_to_split = trial.suggest_float('min_gain_to_split', 0.0, 1.0)\n",
        "            feature_fraction = trial.suggest_float('feature_fraction', 0.5, 1.0)\n",
        "            bagging_fraction = trial.suggest_float('bagging_fraction', 0.5, 1.0)\n",
        "            bagging_freq = trial.suggest_int('bagging_freq', 1, 7)\n",
        "            max_bin = trial.suggest_int('max_bin', 64, 255)\n",
        "\n",
        "            params = {\n",
        "                'n_estimators': 100, # to reduce overfitting and training time\n",
        "                'num_leaves': num_leaves,\n",
        "                # 'max_depth': max_depth,\n",
        "                'learning_rate': learning_rate,\n",
        "                'min_data_in_leaf': min_data_in_leaf,\n",
        "                'min_gain_to_split': min_gain_to_split,\n",
        "                'feature_fraction': feature_fraction,\n",
        "                'bagging_fraction': bagging_fraction,\n",
        "                'bagging_freq': bagging_freq,\n",
        "                'max_bin': max_bin,\n",
        "                # 'objective': 'binary',  # Specify the objective function\n",
        "                # 'metric': 'binary_logloss',  # Specify the evaluation metric\n",
        "                'random_state': self.seeds[self.s],\n",
        "                'n_jobs': self.n_jobs\n",
        "            }\n",
        "\n",
        "            if self.reg:\n",
        "                params.update({\n",
        "                    'lambda_l1': lambda_l1,\n",
        "                    'lambda_l2': lambda_l2,\n",
        "                })\n",
        "\n",
        "            results = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(self.train_and_evaluate)(train_index, test_index, X, y, params)\n",
        "                for train_index, test_index in sss_opt.split(X)\n",
        "            )\n",
        "\n",
        "            return np.mean([result[1] for result in results])\n",
        "\n",
        "        # Mapear model_type a la función objetivo correspondiente\n",
        "        objective_map = {\n",
        "            'xgboost': objective_xgboost,\n",
        "            'lightgbm': objective_lightgbm\n",
        "        }\n",
        "\n",
        "        objective = objective_map[self.model_type]\n",
        "\n",
        "        study = optuna.create_study(\n",
        "            direction=\"maximize\",\n",
        "            study_name=study_name,\n",
        "            storage=storage_name,\n",
        "            load_if_exists=True\n",
        "        )\n",
        "\n",
        "        if optimize:\n",
        "            print(f\"Optimizando {self.model_type} con {n_trials} pruebas\")\n",
        "            study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "        best_trial = study.best_trial\n",
        "        self.best_params = best_trial.params  # Guardar los mejores parámetros\n",
        "\n",
        "        print(f\"Mejores parámetros para {self.model_type}: {best_trial.params}\")\n",
        "        return best_trial.params\n",
        "\n",
        "    def train_base_model(self, X_train, y_train):\n",
        "        classifier_class = self.classifier_map[self.model_type]\n",
        "        self.base_model = classifier_class(**self.base_params)\n",
        "        self.base_model.fit(X_train, y_train)\n",
        "\n",
        "    def train_best_model(self, X_train, y_train):\n",
        "        if self.best_params is None:\n",
        "            print(\"No se encontraron mejores parámetros. Por favor, ejecuta optimize_model primero.\")\n",
        "            return\n",
        "        classifier_class = self.classifier_map[self.model_type]\n",
        "        self.best_model = classifier_class(**self.best_params)\n",
        "        self.best_model.fit(X_train, y_train)\n",
        "\n",
        "    def compare_models(self, X, y):\n",
        "        sss = StratifiedShuffleSplit(n_splits=30, test_size=0.3, random_state=self.seeds[self.s])\n",
        "\n",
        "        results_base = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(self.train_and_evaluate)(train_index, test_index, X, y, self.base_params)\n",
        "            for train_index, test_index in sss.split(X, y)\n",
        "        )\n",
        "        results_best = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(self.train_and_evaluate)(train_index, test_index, X, y, self.best_params)\n",
        "            for train_index, test_index in sss.split(X, y)\n",
        "        )\n",
        "\n",
        "        return results_base, results_best\n",
        "\n",
        "    def plot_comparisons(self, results_base, results_best):\n",
        "        df_pred = pd.DataFrame({'Ganancia': [result[1] for result in results_base], 'Modelo': 'Base'})\n",
        "        df_pred_best = pd.DataFrame({'Ganancia': [result[1] for result in results_best], 'Modelo': 'Best'})\n",
        "        df_combined = pd.concat([df_pred, df_pred_best])\n",
        "\n",
        "        g = sns.FacetGrid(df_combined, row=\"Modelo\", aspect=2)\n",
        "        g.map(sns.histplot, \"Ganancia\", kde=True)\n",
        "        plt.show()\n",
        "\n",
        "        mean_base = df_combined[df_combined['Modelo'] == 'Base']['Ganancia'].mean()\n",
        "        mean_best = df_combined[df_combined['Modelo'] == 'Best']['Ganancia'].mean()\n",
        "\n",
        "        print(f\"Ganancia media del modelo base: {mean_base}\")\n",
        "        print(f\"Ganancia media del modelo Best: {mean_best}\")\n",
        "\n",
        "    def test_model(self, model, X, y):\n",
        "        ganancia_test = self.ganancia(model, X, y)\n",
        "        print(f\"Ganancia del modelo en el conjunto de test: {ganancia_test}\")\n",
        "        return ganancia_test\n",
        "\n",
        "    def test_base_model(self, X, y):\n",
        "        return self.test_model(self.base_model, X, y)\n",
        "\n",
        "    def test_best_model(self, X, y):\n",
        "        return self.test_model(self.best_model, X, y)\n",
        "\n",
        "    def simulate_kaggle_split(self, mes_futuro, imputer=None):\n",
        "        \"\"\"\n",
        "        Simula el split público/privado como en una competencia de Kaggle.\n",
        "        \"\"\"\n",
        "        # Obtener los datos futuros\n",
        "        X_futuro, y_futuro = self.def_xy(mes_futuro, target='clase_ternaria', to_pred=False)\n",
        "        if imputer is not None:\n",
        "            X_futuro = pd.DataFrame(imputer.fit_transform(X_futuro), columns=X_futuro.columns)\n",
        "\n",
        "        # Simular el split público/privado\n",
        "        sss_futuro = StratifiedShuffleSplit(n_splits=50, test_size=0.3, random_state=self.seeds[self.s])\n",
        "\n",
        "        ganancias_futuro_privada_best = []\n",
        "        ganancias_futuro_privada_base = []\n",
        "        ganancias_futuro_publica_best = []\n",
        "        ganancias_futuro_publica_base = []\n",
        "\n",
        "        for train_index, test_index in sss_futuro.split(X_futuro, y_futuro):\n",
        "            # Privado (70% de los datos)\n",
        "            ganancias_futuro_privada_best.append(\n",
        "                self.ganancia(self.best_model, X_futuro.iloc[train_index], y_futuro.iloc[train_index], prop=0.7)\n",
        "            )\n",
        "            ganancias_futuro_privada_base.append(\n",
        "                self.ganancia(self.base_model, X_futuro.iloc[train_index], y_futuro.iloc[train_index], prop=0.7)\n",
        "            )\n",
        "            # Público (30% de los datos)\n",
        "            ganancias_futuro_publica_best.append(\n",
        "                self.ganancia(self.best_model, X_futuro.iloc[test_index], y_futuro.iloc[test_index], prop=0.3)\n",
        "            )\n",
        "            ganancias_futuro_publica_base.append(\n",
        "                self.ganancia(self.base_model, X_futuro.iloc[test_index], y_futuro.iloc[test_index], prop=0.3)\n",
        "            )\n",
        "\n",
        "        # Crear DataFrames para visualización\n",
        "        df_pred_1_best = pd.DataFrame({\n",
        "            'Ganancia': ganancias_futuro_privada_best,\n",
        "            'Modelo': 'Best',\n",
        "            'Grupo': 'Privado'\n",
        "        })\n",
        "        df_pred_2_best = pd.DataFrame({\n",
        "            'Ganancia': ganancias_futuro_publica_best,\n",
        "            'Modelo': 'Best',\n",
        "            'Grupo': 'Publico'\n",
        "        })\n",
        "        df_pred_1_base = pd.DataFrame({\n",
        "            'Ganancia': ganancias_futuro_privada_base,\n",
        "            'Modelo': 'Base',\n",
        "            'Grupo': 'Privado'\n",
        "        })\n",
        "        df_pred_2_base = pd.DataFrame({\n",
        "            'Ganancia': ganancias_futuro_publica_base,\n",
        "            'Modelo': 'Base',\n",
        "            'Grupo': 'Publico'\n",
        "        })\n",
        "\n",
        "        df_combined = pd.concat([df_pred_1_base, df_pred_2_base, df_pred_1_best, df_pred_2_best])\n",
        "\n",
        "        # Visualización\n",
        "        g = sns.FacetGrid(df_combined, col=\"Grupo\", row=\"Modelo\", aspect=2)\n",
        "        g.map(sns.histplot, \"Ganancia\", kde=True)\n",
        "        plt.show()\n",
        "\n",
        "        # Cálculo de ganancias medias\n",
        "        mean_base_privado = df_combined[\n",
        "            (df_combined['Modelo'] == 'Base') & (df_combined['Grupo'] == 'Privado')\n",
        "        ]['Ganancia'].mean()\n",
        "        mean_base_publico = df_combined[\n",
        "            (df_combined['Modelo'] == 'Base') & (df_combined['Grupo'] == 'Publico')\n",
        "        ]['Ganancia'].mean()\n",
        "        mean_best_privado = df_combined[\n",
        "            (df_combined['Modelo'] == 'Best') & (df_combined['Grupo'] == 'Privado')\n",
        "        ]['Ganancia'].mean()\n",
        "        mean_best_publico = df_combined[\n",
        "            (df_combined['Modelo'] == 'Best') & (df_combined['Grupo'] == 'Publico')\n",
        "        ]['Ganancia'].mean()\n",
        "\n",
        "        print(f\"Ganancia media del modelo base en privado: {mean_base_privado}\")\n",
        "        print(f\"Ganancia media del modelo base en público: {mean_base_publico}\")\n",
        "        print(f\"Ganancia media del modelo Best en privado: {mean_best_privado}\")\n",
        "        print(f\"Ganancia media del modelo Best en público: {mean_best_publico}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost \n",
        "\n",
        "**Aggressive FE 3 + data cleansing**\n",
        "\n",
        "La creación del target y el análisis exploratorio, está avanzado en el Notebook_comp01.\n",
        "\n",
        "Luego, un proceso de **feature engineering conceptual y extendido a 3 meses previos**, fue llevado a cabo en el Notebook_comp01_fe_agr_3.\n",
        "\n",
        "Agrego, la eliminación de features conflictivas y regularización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\santt\\AppData\\Local\\Temp\\ipykernel_24104\\217547193.py:5: DtypeWarning: Columns (154) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(dataset_file)\n"
          ]
        }
      ],
      "source": [
        "dataset_file = 'competencia_01_brandoni_fe_agr_3.csv'\n",
        "\n",
        "dataset_file = os.path.join(datos, dataset_file)\n",
        "\n",
        "data = pd.read_csv(dataset_file)\n",
        "\n",
        "# data cleansing + data drifting\n",
        "to_drop = ['Master_fultimo_cierre', 'Visa_fultimo_cierre'] + ['cprestamos_personales', 'mprestamos_personales']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uso del pipeline con XGBoost\n",
        "\n",
        "En abril\n",
        "\n",
        "Sin mes_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapear etiquetas de clase a números\n",
        "label_mapping = {'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2}\n",
        "\n",
        "data['clase_ternaria'] = data['clase_ternaria'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "202104"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mes_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Corriendo pipeline con XGBoost ###\n",
            "Columnas con todos los valores NaN: ['payroll_slope_1_cliente_antiguedad', 'cuenta_corriente_slope_1_cliente_antiguedad', 'visa_consumo_slope_1_cliente_antiguedad', 'comisiones_mantenimiento_slope_1_cliente_antiguedad', 'comisiones_otras_slope_1_cliente_antiguedad', 'payroll_slope_1_foto_mes', 'cuenta_corriente_slope_1_foto_mes', 'visa_consumo_slope_1_foto_mes', 'comisiones_mantenimiento_slope_1_foto_mes', 'comisiones_otras_slope_1_foto_mes']\n",
            "\n",
            "# Optimizando el modelo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-10-28 11:55:05,061] Using an existing study with name 'comp01_v02_pipeline_training_xgboost_opt_2_fe_agr_3_reg' instead of creating a new one.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores parámetros para xgboost: {'max_leaves': 19, 'eta': 0.11537876667197919, 'gamma': 2.9208841946871176, 'min_child_weight': 7, 'subsample': 0.9416959858943643, 'colsample_bytree': 0.5429812768188057, 'lambda': 7.226815448281549, 'alpha': 0.5929685712726126, 'scale_pos_weight': 8.669078758370219}\n",
            "\n",
            "# Entrenando el mejor modelo con parámetros optimizados\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\santt\\.conda\\envs\\dm_eyf\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:55:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
            "Parameters: { \"scale_pos_weight\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Semilla a usar\n",
        "s = 1\n",
        "\n",
        "print(\"### Corriendo pipeline con XGBoost ###\")\n",
        "# Inicializar el pipeline con 'xgboost'\n",
        "pipeline_xgb_reg = ModelPipeline(data, semillas, model_type='xgboost', \n",
        "                             seed=s, n_jobs=2, reg=True)\n",
        "\n",
        "X_train, y_train = pipeline_xgb_reg.def_xy(mes_train)\n",
        "\n",
        "# Identificar y eliminar columnas con todos los valores NaN\n",
        "cols_with_all_nan = X_train.columns[X_train.isna().all()]\n",
        "print(\"Columnas con todos los valores NaN:\", cols_with_all_nan.tolist())\n",
        "\n",
        "# Drop these columns + bad columns (DQ + DD)\n",
        "to_drop = list(cols_with_all_nan) + to_drop\n",
        "X_train = X_train.drop(columns=to_drop)\n",
        "\n",
        "# Imputación de valores faltantes\n",
        "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "X_train_imp = pd.DataFrame(imp_median.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "# Identificar variables categóricas\n",
        "categorical_features = [col for col in X_train_imp.columns if X_train_imp[col].dtype == 'object']\n",
        "\n",
        "# Identificar variables categóricas\n",
        "categorical_features = X_train_imp.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Convertir columnas categóricas al tipo 'category'\n",
        "for col in categorical_features:\n",
        "    X_train_imp[col] = X_train_imp[col].astype('category')\n",
        "\n",
        "# print(\"\\n# Entrenando el modelo base XGBoost\")\n",
        "# pipeline_xgb_reg.train_base_model(X_train_imp, y_train)\n",
        "\n",
        "# Definir el almacenamiento para Optuna\n",
        "storage_xgb = \"sqlite:///optimizacion/optimization_tree.db\"\n",
        "study_xgb = \"comp01_v02_pipeline_training_xgboost_opt_2_fe_agr_3_reg\"\n",
        "\n",
        "print(\"\\n# Optimizando el modelo\")\n",
        "pipeline_xgb_reg.optimize_model(\n",
        "    X_train_imp, y_train,\n",
        "    storage_name=storage_xgb,\n",
        "    study_name=study_xgb,\n",
        "    optimize=False,  # Establecer en True para realizar la optimización\n",
        "    n_trials=100  # Ajusta el número de pruebas según sea necesario\n",
        ")\n",
        "\n",
        "# Entrenar el mejor modelo con parámetros optimizados\n",
        "print(\"\\n# Entrenando el mejor modelo con parámetros optimizados\")\n",
        "pipeline_xgb_reg.train_best_model(X_train_imp, y_train)\n",
        "\n",
        "# # Comparar modelos\n",
        "# print(\"\\n# Comparando modelos\")\n",
        "# results_base_xgb_reg, results_best_xgb_reg = pipeline_xgb_reg.compare_models(X_train_imp, y_train)\n",
        "# pipeline_xgb_reg.plot_comparisons(results_base_xgb_reg, results_best_xgb_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'objective': 'multi:softprob',\n",
              " 'base_score': None,\n",
              " 'booster': None,\n",
              " 'callbacks': None,\n",
              " 'colsample_bylevel': None,\n",
              " 'colsample_bynode': None,\n",
              " 'colsample_bytree': 0.5429812768188057,\n",
              " 'device': None,\n",
              " 'early_stopping_rounds': None,\n",
              " 'enable_categorical': False,\n",
              " 'eval_metric': None,\n",
              " 'feature_types': None,\n",
              " 'gamma': 2.9208841946871176,\n",
              " 'grow_policy': None,\n",
              " 'importance_type': None,\n",
              " 'interaction_constraints': None,\n",
              " 'learning_rate': None,\n",
              " 'max_bin': None,\n",
              " 'max_cat_threshold': None,\n",
              " 'max_cat_to_onehot': None,\n",
              " 'max_delta_step': None,\n",
              " 'max_depth': None,\n",
              " 'max_leaves': 19,\n",
              " 'min_child_weight': 7,\n",
              " 'missing': nan,\n",
              " 'monotone_constraints': None,\n",
              " 'multi_strategy': None,\n",
              " 'n_estimators': None,\n",
              " 'n_jobs': None,\n",
              " 'num_parallel_tree': None,\n",
              " 'random_state': None,\n",
              " 'reg_alpha': None,\n",
              " 'reg_lambda': None,\n",
              " 'sampling_method': None,\n",
              " 'scale_pos_weight': 8.669078758370219,\n",
              " 'subsample': 0.9416959858943643,\n",
              " 'tree_method': None,\n",
              " 'validate_parameters': None,\n",
              " 'verbosity': None,\n",
              " 'eta': 0.11537876667197919,\n",
              " 'lambda': 7.226815448281549,\n",
              " 'alpha': 0.5929685712726126}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_xgb_reg.best_model.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensamble\n",
        "\n",
        "XGBoost + Redes neuronales + Atención"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usando el XGBoost pre-entrenado, vamos a desarrollar un modelo de ensamble con redes neuronales y mecanismos de atención."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. XGBoost re-entrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\santt\\AppData\\Local\\Temp\\ipykernel_24104\\217547193.py:5: DtypeWarning: Columns (154) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(dataset_file)\n"
          ]
        }
      ],
      "source": [
        "dataset_file = 'competencia_01_brandoni_fe_agr_3.csv'\n",
        "\n",
        "dataset_file = os.path.join(datos, dataset_file)\n",
        "\n",
        "data = pd.read_csv(dataset_file)\n",
        "\n",
        "# data cleansing + data drifting\n",
        "to_drop = ['Master_fultimo_cierre', 'Visa_fultimo_cierre'] + ['cprestamos_personales', 'mprestamos_personales']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Target ternaria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_multi = 'clase_ternaria'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data[data['foto_mes'] == mes_train]\n",
        "y = X[target_multi]\n",
        "X = X.drop(columns=[target_multi])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapear etiquetas de clase a números\n",
        "label_mapping = {'CONTINUA': 0, 'BAJA+1': 1, 'BAJA+2': 2}\n",
        "\n",
        "y = y.map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "clase_ternaria\n",
              "0    161919\n",
              "2      1189\n",
              "1       982\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Target binaria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_bin = pd.Series(\n",
        "                np.where(y == 2, 1, 0),\n",
        "                name='clase_binaria',\n",
        "                index=y.index\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "clase_binaria\n",
              "0    162901\n",
              "1      1189\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_bin.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split del train para tomar una porción para validación y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                    # y, # ternaria\n",
        "                                                    y_bin, # binaria\n",
        "                                                    test_size=0.1,\n",
        "                                                    # stratify=y,\n",
        "                                                    stratify=y_bin, \n",
        "                                                    random_state=semillas[1])\n",
        "\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(alpha=0.5929685712726126, base_score=None, booster=None,\n",
              "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.5429812768188057, device=None,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eta=0.11537876667197919, eval_metric=None, feature_types=None,\n",
              "              gamma=2.9208841946871176, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, lambda=7.226815448281549,\n",
              "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
              "              max_leaves=19, min_child_weight=7, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(alpha=0.5929685712726126, base_score=None, booster=None,\n",
              "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.5429812768188057, device=None,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eta=0.11537876667197919, eval_metric=None, feature_types=None,\n",
              "              gamma=2.9208841946871176, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, lambda=7.226815448281549,\n",
              "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
              "              max_leaves=19, min_child_weight=7, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=None, ...)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "XGBClassifier(alpha=0.5929685712726126, base_score=None, booster=None,\n",
              "              callbacks=None, colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.5429812768188057, device=None,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eta=0.11537876667197919, eval_metric=None, feature_types=None,\n",
              "              gamma=2.9208841946871176, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, lambda=7.226815448281549,\n",
              "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
              "              max_leaves=19, min_child_weight=7, missing=nan,\n",
              "              monotone_constraints=None, multi_strategy=None, n_estimators=None, ...)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tomo los parámetros óptimos del modelo a trabajar\n",
        "opt_params = pipeline_xgb_reg.best_model.get_params()\n",
        "\n",
        "opt_params.update({'objective': 'binary:logistic'}) # para binario\n",
        "\n",
        "# instancia con mejores parámetros\n",
        "xgb_clf = XGBClassifier(**opt_params, seed=semillas[1])\n",
        "\n",
        "# entreno\n",
        "xgb_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trees: 100\n",
            "Model Parameters: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.5429812768188057, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': 2.9208841946871176, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': 19, 'min_child_weight': 7, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': 8.669078758370219, 'subsample': 0.9416959858943643, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'eta': 0.11537876667197919, 'lambda': 7.226815448281549, 'alpha': 0.5929685712726126, 'seed': 327347}\n"
          ]
        }
      ],
      "source": [
        "# Get number of trees in the model\n",
        "num_trees = xgb_clf.get_booster().num_boosted_rounds()\n",
        "print(\"Number of trees:\", num_trees)\n",
        "\n",
        "# Get the model parameters\n",
        "params = xgb_clf.get_params()\n",
        "print(\"Model Parameters:\", params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluation Libraries\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "def find_best_threshold(y_pred_prob, y_test, best_thresh = None):\n",
        "    \"\"\"\n",
        "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
        "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
        "\n",
        "    dtype model: xgboost.sklearn.XGBClassifier\n",
        "    dtype x_list: list or array\n",
        "    dtype y_test: np.array\n",
        "    dtype best_thresh: float\n",
        "    rtype best_f1: float\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set threshold range as [0.1, 0.2, ..., 0.5]. \n",
        "    threshold_list = np.arange(0.1,0.6,0.1)\n",
        "    # Set an initial value of best threshold.\n",
        "    best_f1 = 0\n",
        "    \n",
        "    # If best_thresh is set as \"None\", this function is to find the best_thresh as well as best_f1 \n",
        "    if best_thresh ==None:\n",
        "        for th in threshold_list:\n",
        "            y_pred_label = (y_pred_prob > th)*1 \n",
        "            f_score = f1_score(y_test,y_pred_label)\n",
        "            if f_score > best_f1:\n",
        "                best_f1 = f_score\n",
        "                best_thresh = th \n",
        "        return best_thresh, best_f1, roc_auc_score(y_test, y_pred_prob)\n",
        "    \n",
        "    # If best_thresh is set as a certain number, this function is to calculate its f1 score.\n",
        "    else:\n",
        "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
        "        best_f1 = f1_score(y_test,y_pred_label)\n",
        "        \n",
        "    return best_f1, roc_auc_score(y_test, y_pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testeo de compromiso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------Evaluating xgboost model------\n",
            "AUC = 0.9220, F1-score = 0.2483\n"
          ]
        }
      ],
      "source": [
        "# Evaluate xgboost model\n",
        "print(\"------Evaluating xgboost model------\")\n",
        "# Predict\n",
        "test_pred = xgb_clf.predict_proba(X_test)[:,1]\n",
        "# Calculate auc\n",
        "xgb_auc = roc_auc_score(y_test, test_pred)\n",
        "\n",
        "# Find the best threshold\n",
        "xgb_threshold,_,_ = find_best_threshold(xgb_clf.predict_proba(X_test)[:,1], y_test)\n",
        "# Calculate the best f1 score\n",
        "xgb_f1,_ = find_best_threshold(xgb_clf.predict_proba(X_test)[:,1], y_test, best_thresh = xgb_threshold)\n",
        "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgb_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Guardado del modelo XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['modelos/xgb_classifier_model.pkl']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "xgb_clf_path = 'modelos/xgb_classifier_model.pkl'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(xgb_clf, xgb_clf_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Funciones de ensamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**process_lead_idx** function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_leaf_idx(X_leaves): \n",
        "    \"\"\"\n",
        "    This function is to convert the output of XGBoost model to the input of DATE model.\n",
        "    For an individual import, the output of XGBoost model is a list of leaf index of multiple trees.\n",
        "    eg. [1, 1, 10, 9, 30, 30, 32, ... ]\n",
        "    How to distinguish \"node 1\" of the first tree from \"node 1\" of the second tree?\n",
        "    How to distinguish \"node 30\" of the fifth tree from \"node 30\" of the sixth tree?\n",
        "    This function is to assign unique index to every leaf node in all the trees. \n",
        "    \n",
        "    This function returns;\n",
        "    - lists of unique leaf index;\n",
        "    - total number of unique leaf nodes; and\n",
        "    - a reference table (dictionary) composed of \"unique leaf index\", \"tree id\", \"(previous) leaf index\".\n",
        "    \n",
        "    dtype X_leaves: np.array\n",
        "    rtype leaves: list\n",
        "    rtype total_leaves: int\n",
        "    rtype new_leaf_index: dict \n",
        "    \"\"\"\n",
        "    leaves = X_leaves.copy()\n",
        "    new_leaf_index = dict() # Dictionary to store leaf index\n",
        "    total_leaves = 0\n",
        "    for c in range(X_leaves.shape[1]): # Iterate for each column (i.e. 100 trees)\n",
        "        column = X_leaves[:,c]\n",
        "        unique_vals = list(sorted(set(column)))\n",
        "        new_idx = {v:(i+total_leaves) for i,v in enumerate(unique_vals)}\n",
        "        for i,v in enumerate(unique_vals):\n",
        "            leaf_id = i+total_leaves\n",
        "            new_leaf_index[leaf_id] = {c:v}\n",
        "        leaves[:,c] = [new_idx[v] for v in column]\n",
        "        total_leaves += len(unique_vals)\n",
        "        \n",
        "    assert leaves.ravel().max() == total_leaves - 1\n",
        "    return leaves,total_leaves,new_leaf_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**fgsm_attack** function\n",
        "\n",
        "This function is to generate adversarial samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fgsm_attack(model, loss, images, labels, eps):\n",
        "    \"\"\"\n",
        "    Generate adversarial examples\n",
        "    \n",
        "    dtype model: torch.nn.parallel.data_parallel.DataParallel\n",
        "    dtype loss: torch.nn.modules.loss.BCELoss\n",
        "    dtype images: torch.Tensor\n",
        "    dtype labels: torch.Tensor\n",
        "    dtype eps: float\n",
        "    rtype attack_images: torch.Tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    images = Variable(images, requires_grad=True)\n",
        "    outputs = model.module.pred_from_hidden(images)\n",
        "    \n",
        "    model.zero_grad()\n",
        "    cost = loss(outputs, labels)\n",
        "    cost.backward()\n",
        "    attack_images = images + eps * images.grad.sign()\n",
        "    \n",
        "    return attack_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**metrics** function\n",
        "\n",
        "This is the function which generates metrics to evaluate the performance of the model.  \n",
        "\n",
        "For date model, we will calculate the metrics of evaluation for each group of 1%, 2%, 5%, and 10% suspicious transactions suggested by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metrics(y_prob,xgb_testy,best_thresh=None):\n",
        "    \"\"\"\n",
        "    Generate metrics for evaluation\n",
        "    \n",
        "    dtype y_prob: np.array\n",
        "    dtype xgb_testy: np.array\n",
        "    dtype revenue_test: np.array\n",
        "    dtype best_thresh: float64\n",
        "    rtype overall_f1: float64\n",
        "    rtype auc: float64\n",
        "    rtype pr: list\n",
        "    rtype re: list\n",
        "    rtype f: list\n",
        "    rtype rev: list\n",
        "    \"\"\"\n",
        "    \n",
        "    if best_thresh ==None:\n",
        "        _,overall_f1,auc = find_best_threshold(y_prob,xgb_testy,best_thresh)\n",
        "    else:\n",
        "        overall_f1,auc = find_best_threshold(y_prob,xgb_testy,best_thresh)\n",
        "    # Seized revenue \n",
        "    # Precision and Recall\n",
        "    pr, re, f = [], [], []\n",
        "    for i in [99,98,95,90]:\n",
        "        threshold = np.percentile(y_prob, i)\n",
        "\n",
        "        precision = np.mean(xgb_testy[y_prob > threshold])\n",
        "        recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
        "        f1 = 2*precision*recall/(precision+recall)\n",
        "\n",
        "        # Save results\n",
        "        pr.append(precision)\n",
        "        re.append(recall)\n",
        "        f.append(f1)\n",
        "    \n",
        "    return overall_f1, auc, pr, re, f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Leaf nodes of each import from XGB model**\n",
        "\n",
        "Get output values from the XGBoost model and convert them to the input for NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get leaf index from xgboost model \n",
        "X_train_leaves = xgb_clf.apply(X_train) #apply: Return the predicted leaf every tree for each sample.\n",
        "# X_valid_leaves = xgb_clf.apply(X_val)\n",
        "X_test_leaves = xgb_clf.apply(X_test)\n",
        "train_rows = X_train_leaves.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "train_rows = X_train.shape[0]\n",
        "test_rows = X_test.shape[0] + train_rows\n",
        "\n",
        "# Convert output values of the XGBoost model to the input form of the DATE model.\n",
        "X_leaves = np.concatenate((X_train_leaves, X_test_leaves), axis=0) # Make sure the dimensionality\n",
        "\n",
        "transformed_leaves, leaf_num, new_leaf_index = process_leaf_idx(X_leaves)\n",
        "\n",
        "train_leaves, test_leaves = transformed_leaves[:train_rows],\\\n",
        "                                          transformed_leaves[train_rows:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1888"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number \n",
        "len(new_leaf_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   5.,   22.,   41., ..., 1842., 1858., 1872.],\n",
              "       [  10.,   27.,   48., ..., 1833., 1860., 1886.],\n",
              "       [  10.,   27.,   48., ..., 1842., 1859., 1870.],\n",
              "       ...,\n",
              "       [   9.,   28.,   45., ..., 1843., 1859., 1872.],\n",
              "       [  11.,   28.,   48., ..., 1842., 1862., 1873.],\n",
              "       [  11.,   28.,   48., ..., 1842., 1859., 1873.]], dtype=float32)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformed_leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.000e+00, 2.200e+01, 4.100e+01, ..., 1.842e+03, 1.858e+03,\n",
              "        1.872e+03],\n",
              "       [1.000e+01, 2.700e+01, 4.800e+01, ..., 1.833e+03, 1.860e+03,\n",
              "        1.886e+03],\n",
              "       [1.000e+01, 2.700e+01, 4.800e+01, ..., 1.842e+03, 1.859e+03,\n",
              "        1.870e+03],\n",
              "       ...,\n",
              "       [1.000e+01, 2.600e+01, 5.000e+01, ..., 1.842e+03, 1.859e+03,\n",
              "        1.872e+03],\n",
              "       [1.000e+00, 2.000e+01, 3.900e+01, ..., 1.843e+03, 1.859e+03,\n",
              "        1.872e+03],\n",
              "       [1.000e+01, 2.700e+01, 4.500e+01, ..., 1.842e+03, 1.859e+03,\n",
              "        1.872e+03]], dtype=float32)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   9.,   26.,   48., ..., 1835., 1860., 1873.],\n",
              "       [  10.,   25.,   43., ..., 1842., 1859., 1872.],\n",
              "       [  11.,   28.,   48., ..., 1833., 1858., 1875.],\n",
              "       ...,\n",
              "       [   9.,   28.,   45., ..., 1843., 1859., 1872.],\n",
              "       [  11.,   28.,   48., ..., 1842., 1862., 1873.],\n",
              "       [  11.,   28.,   48., ..., 1842., 1859., 1873.]], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_leaves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seteo de semilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data as Data\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # Si usas múltiples GPUs\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(semillas[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Convert data to tensor\n",
        "\n",
        "Tensor is a collection of numbers with specific shape (dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to torch type\n",
        "train_leaves = torch.tensor(train_leaves).long()\n",
        "test_leaves = torch.tensor(test_leaves).long()\n",
        "\n",
        "# cls data\n",
        "train_label_cls = torch.tensor(y_train).float()\n",
        "test_label_cls = torch.tensor(y_test).float()\n",
        "\n",
        "# Create dataset \n",
        "train_dataset = Data.TensorDataset(train_leaves, train_label_cls)\n",
        "test_dataset = Data.TensorDataset(test_leaves, test_label_cls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Create dataloader\n",
        "batch_size = 128\n",
        "train_loader = Data.DataLoader(\n",
        "    dataset=train_dataset,     \n",
        "    batch_size=batch_size,      \n",
        "    shuffle=True,               \n",
        ")\n",
        "\n",
        "test_loader = Data.DataLoader(\n",
        "    dataset=test_dataset,     \n",
        "    batch_size=batch_size,      \n",
        "    shuffle=False,               \n",
        ")\n",
        "\n",
        "# Model information\n",
        "curr_time = str(time.time())\n",
        "model_name = \"DMEyF_ensamble\"\n",
        "model_path = \"modelos/%s%s.pkl\" % (model_name, curr_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch-multi-head-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch_multi_head_attention import MultiHeadAttention\n",
        "from torch.autograd import Variable\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Attención"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention sends all outputs of the encoder to the decoder, and at each time step, \n",
        "    the memory cell of the decoder calculates the sum of the weights from all encoder outputs \n",
        "    to determine which data to focus more on.\n",
        "    \"\"\"\n",
        "    def __init__(self,dim,hidden,aggregate=\"sum\"):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_matrix = nn.Linear(dim, hidden)\n",
        "        self.project_weight = nn.Linear(hidden*2, hidden)\n",
        "        self.h = nn.Parameter(torch.rand(hidden,1))\n",
        "        self.agg_type = aggregate\n",
        "        \n",
        "    def forward(self, query, key): # Query: 256 X 16, # key: 256 X 100 X 16, # Assume key==value\n",
        "        dim = query.size(-1) # 16 (n_embedding_dimension)\n",
        "        batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
        "        time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
        "        \n",
        "        # Concate input query and key \n",
        "        query = query.view(batch, 1, dim) # View = reshape: (256X16) -> (256X1X16)\n",
        "        query = query.expand(batch, time_step, -1) # Expand to the same dimension: (256X1X16) -> (256X100X16)\n",
        "        cat_vector = torch.cat((query, key), dim=-1) # (256X100X32)\n",
        "        \n",
        "        # Project to single value\n",
        "        project_vector = self.project_weight(cat_vector) \n",
        "        project_vector = torch.relu(project_vector)\n",
        "        attention_alpha = torch.matmul(project_vector, self.h)\n",
        "        attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
        "        attention_vec = key * attention_weight\n",
        "        \n",
        "        # Aggregate leaves\n",
        "        if self.agg_type == \"max\":\n",
        "            attention_vec, _ = torch.max(attention_vec, dim=1)\n",
        "        elif self.agg_type ==\"mean\":\n",
        "            attention_vec = torch.mean(attention_vec, dim=1)\n",
        "        elif self.agg_type ==\"sum\":\n",
        "            attention_vec = torch.sum(attention_vec, dim=1)\n",
        "        return attention_vec, attention_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensamble DMEyF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con normalización extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DMEyF_norm(nn.Module):\n",
        "    \"\"\"\n",
        "    Esta es la clase para crear la estructura completa del modelo DATE\n",
        "    \"\"\"\n",
        "    def __init__(self, max_leaf, dim, head_num=4, act=\"relu\", \n",
        "                 device=\"cuda\", use_self=True, use_att=True, agg_type=\"sum\"):\n",
        "        super(DMEyF_norm, self).__init__()\n",
        "        self.d = dim\n",
        "        self.device = device\n",
        "        if act == \"relu\":\n",
        "            self.act = nn.LeakyReLU()\n",
        "        self.use_self = use_self\n",
        "        self.use_att = use_att\n",
        "        self.agg_type = agg_type\n",
        "        # Capas de embedding\n",
        "        self.leaf_embedding = nn.Embedding(max_leaf, dim)\n",
        "\n",
        "        # Capas de atención\n",
        "        self.attention_block = Attention(dim, dim, self.agg_type).to(device)\n",
        "        self.self_att = MultiHeadAttention(dim, head_num).to(device)\n",
        "\n",
        "        # Capas de normalización para las salidas de atención\n",
        "        self.layer_norm_self_att = nn.LayerNorm(dim)\n",
        "        self.layer_norm_att_block = nn.LayerNorm(dim)\n",
        "\n",
        "        # Capas ocultas y de salida con normalización\n",
        "        self.hidden1 = nn.Linear(dim, 128)\n",
        "        self.hidden2 = nn.Linear(128, 256)\n",
        "        self.hidden3 = nn.Linear(256, dim)\n",
        "        self.output_cls_layer = nn.Linear(dim, 1)\n",
        "    \n",
        "    def forward(self, feature):\n",
        "        # Embedding de leaf_id\n",
        "        leaf_vectors = self.leaf_embedding(feature)\n",
        "        \n",
        "        # Primera atención: Multi-Head Self-Attention\n",
        "        if self.use_self:\n",
        "            leaf_vectors = self.self_att(leaf_vectors, leaf_vectors, leaf_vectors)\n",
        "            # Normalización después de self_att\n",
        "            leaf_vectors = self.layer_norm_self_att(leaf_vectors)\n",
        "        \n",
        "        if self.use_att:\n",
        "            # Computar la media de leaf_vectors para obtener query_vector\n",
        "            query_vector = torch.mean(leaf_vectors, dim=1)  # (batch_size, dim)\n",
        "            # Segunda atención: Atención con un vector de consulta propio\n",
        "            set_vector, self.attention_w = self.attention_block(query_vector, leaf_vectors)\n",
        "            # Normalización después de attention_block\n",
        "            set_vector = self.layer_norm_att_block(set_vector)\n",
        "        else:\n",
        "            # Agregar leaves\n",
        "            if self.agg_type == \"max\":\n",
        "                set_vector, _ = torch.max(leaf_vectors, dim=1)\n",
        "            elif self.agg_type ==\"mean\":\n",
        "                set_vector = torch.mean(leaf_vectors, dim=1)\n",
        "            elif self.agg_type ==\"sum\":\n",
        "                set_vector = torch.sum(leaf_vectors, dim=1)\n",
        "\n",
        "            # Normalización del set_vector si no se usa atención\n",
        "            set_vector = self.layer_norm_att_block(set_vector)\n",
        "    \n",
        "        # Pasar por las capas ocultas con activaciones y normalización\n",
        "        hidden = self.act(self.hidden1(set_vector))\n",
        "        hidden = self.act(self.hidden2(hidden))\n",
        "        hidden = self.act(self.hidden3(hidden))\n",
        "\n",
        "        # Salida de clasificación\n",
        "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
        "        return classification_output, hidden\n",
        "\n",
        "    def pred_from_hidden(self, hidden):\n",
        "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
        "        return classification_output \n",
        "\n",
        "    def eval_on_batch(self, test_loader):  # Predecir datos de prueba usando batch\n",
        "        final_output = []\n",
        "        cls_loss = []\n",
        "        for batch in test_loader:\n",
        "            batch_feature, batch_cls = batch\n",
        "            batch_feature, batch_cls = batch_feature.to(self.device), batch_cls.to(self.device)\n",
        "            batch_cls = batch_cls.view(-1,1)\n",
        "            y_pred_prob, _ = self.forward(batch_feature)\n",
        "\n",
        "            # Calcular pérdida de clasificación\n",
        "            cls_losses = nn.BCELoss()(y_pred_prob, batch_cls)\n",
        "            cls_loss.append(cls_losses.item())\n",
        "\n",
        "            # Almacenar probabilidad predicha\n",
        "            y_pred = y_pred_prob.detach().cpu().numpy().tolist()\n",
        "            final_output.extend(y_pred)\n",
        "\n",
        "        print(\"Pérdida CLS: %.4f\" % np.mean(cls_loss))\n",
        "        return np.array(final_output).ravel(), np.mean(cls_loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--save'], dest='save', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help='save model or not', metavar=None)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parse argument\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model_name', \n",
        "                        type=str, \n",
        "                        default=\"DMEyF_SST\", \n",
        "                        help=\"Name of model\",\n",
        "                        )\n",
        "parser.add_argument('--epoch', \n",
        "                        type=int, \n",
        "                        default=10, \n",
        "                        help=\"Number of epochs\",\n",
        "                        )\n",
        "parser.add_argument('--dim', \n",
        "                        type=int, \n",
        "                        default=16, \n",
        "                        help=\"Hidden layer dimension\",\n",
        "                        )\n",
        "parser.add_argument('--lr', \n",
        "                        type=float, \n",
        "                        default=0.1, \n",
        "                        help=\"learning rate\",\n",
        "                        )\n",
        "parser.add_argument('--l2',\n",
        "                        type=float,\n",
        "                        default=0.01, #it was 0.001\n",
        "                        help=\"l2 reg\",\n",
        "                        )\n",
        "# parser.add_argument('--alpha',\n",
        "#                         type=float,\n",
        "#                         default=10,\n",
        "#                         help=\"Regression loss weight\",\n",
        "#                         )\n",
        "\n",
        "parser.add_argument('--beta', type=float, default=0.001, help=\"Adversarial loss weight\")\n",
        "parser.add_argument('--head_num', type=int, default=8, help=\"Number of heads for self attention\")\n",
        "parser.add_argument('--use_self', type=int, default=True, help=\"Wheter to use self attention\")\n",
        "parser.add_argument('--use_att', type=int, default=True, help=\"Wheter to use attention\")\n",
        "# parser.add_argument('--fusion', type=str, choices=[\"concat\",\"attention\"], default=\"concat\", help=\"Fusion method for final embedding\")\n",
        "parser.add_argument('--agg', type=str, choices=[\"sum\",\"max\",\"mean\"], default=\"sum\", help=\"Aggreate type for leaf embedding\")\n",
        "parser.add_argument('--act', type=str, choices=[\"relu\"], default=\"relu\", help=\"Activation function\")\n",
        "parser.add_argument('--device', type=str, choices=[\"cuda\",\"cpu\"], default=\"cuda\", help=\"device name for training\")\n",
        "parser.add_argument('--output', type=str, default=\"full.csv\", help=\"Name of output file\")\n",
        "parser.add_argument('--save', type=int, default=0, help=\"save model or not\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "args = parser.parse_args([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Namespace(model_name='DMEyF_SST', epoch=10, dim=16, lr=0.1, l2=0.01, beta=0.001, head_num=8, use_self=True, use_att=True, agg='sum', act='relu', device='cuda', output='full.csv', save=0)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### C. Entrenamiento de prueba\n",
        "\n",
        "Con self_att & att y con extra norm\n",
        "\n",
        "Agg sum\n",
        "\n",
        "Con Balance de clases, penalizando la pérdida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pesos de clase:\n",
            "clase_binaria\n",
            "0    0.007246\n",
            "1    0.992754\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "clases_freq = y_bin.value_counts(normalize=True)\n",
        "\n",
        "class_weights = 1.0 / clases_freq\n",
        "\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "\n",
        "print(\"Pesos de clase:\")\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train(args):\n",
        "    \"\"\"\n",
        "    Train the DATE model with predefined hyperparameters\n",
        "    \n",
        "    dtype *args: strings\n",
        "    \"\"\"\n",
        "    # Get configs\n",
        "    epochs = args.epoch\n",
        "    dim = args.dim\n",
        "    lr = args.lr\n",
        "    weight_decay = args.l2\n",
        "    head_num = args.head_num\n",
        "    device = args.device\n",
        "    act = args.act\n",
        "    beta = args.beta\n",
        "    use_self = True\n",
        "    use_att = True\n",
        "    agg = \"sum\"\n",
        "    \n",
        "    model = DMEyF_norm(leaf_num, dim, head_num, act=act, device=device,\n",
        "                  use_self=use_self, use_att=use_att, agg_type=agg).to(device)\n",
        "    \n",
        "    model = nn.DataParallel(model, device_ids=[0])\n",
        "\n",
        "    # Params measurement\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    # print(model.name)\n",
        "    print(f'Params to be trained: {params}')\n",
        "    # Initialize parameters\n",
        "    # Fills the input Tensor with values according to the method described in \n",
        "    # Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), \n",
        "    # Using a uniform distribution.\n",
        "    \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    # Balanceo de clases por penalizacion de loss\n",
        "    class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float).to(device)\n",
        "\n",
        "    # Optimizer & loss \n",
        "    optimizer = Adam(model.parameters(), weight_decay=weight_decay, lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    cls_loss_func = nn.BCELoss(weight=class_weights_tensor[1])\n",
        "    # cls_loss_func = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor[1])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for step, (batch_feature,batch_cls) in enumerate(train_loader):\n",
        "            model.train() # prep to train model\n",
        "            batch_feature, batch_cls = batch_feature.to(device), batch_cls.to(device)\n",
        "            batch_cls = batch_cls.view(-1,1)\n",
        "\n",
        "            # Model output\n",
        "            classification_output, hidden_vector = model(batch_feature)\n",
        "\n",
        "            # FGSM attack\n",
        "            adv_vector = fgsm_attack(model, cls_loss_func, hidden_vector, batch_cls, 0.01)\n",
        "            adv_output = model.module.pred_from_hidden(adv_vector) \n",
        "\n",
        "            # Calculate loss\n",
        "            adv_loss_func = nn.BCELoss(weight=batch_cls)\n",
        "            # adv_loss_func = nn.BCEWithLogitsLoss(pos_weight=batch_cls)\n",
        "            adv_loss = beta * adv_loss_func(adv_output, batch_cls) \n",
        "            cls_loss = cls_loss_func(classification_output, batch_cls)\n",
        "            loss = cls_loss + adv_loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # if (step+1) % 1000 == 0:  \n",
        "            #     print(\"CLS loss:%.4f, ADV loss:%.4f, Loss:%.4f\"%(cls_loss.item(), adv_loss.item(),loss.item()))\n",
        "                \n",
        "        # Evaluate \n",
        "        model.eval()\n",
        "\n",
        "        print(\"Train at epoch %s\"%(epoch+1))\n",
        "        _, train_loss = model.module.eval_on_batch(train_loader)\n",
        "\n",
        "        print(\"Test at epoch %s\"%(epoch+1))\n",
        "        _, test_loss = model.module.eval_on_batch(test_loader)\n",
        "\n",
        "        # best_threshold, test_score, roc = find_best_threshold(y_prob, y_test)\n",
        "        # overall_f1, auc, precisions, recalls, f1s = metrics(y_prob, y_test, best_threshold)\n",
        "        # select_best = np.mean(f1s)\n",
        "        # print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f, Loss:%.4f\"% (overall_f1, auc, select_best, test_loss))\n",
        "\n",
        "        scheduler.step(test_loss)   \n",
        "        \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to be trained: 71505\n",
            "Train at epoch 1\n",
            "Pérdida CLS: 0.0430\n",
            "Test at epoch 1\n",
            "Pérdida CLS: 0.0428\n",
            "Train at epoch 2\n",
            "Pérdida CLS: 0.0470\n",
            "Test at epoch 2\n",
            "Pérdida CLS: 0.0469\n",
            "Train at epoch 3\n",
            "Pérdida CLS: 0.0534\n",
            "Test at epoch 3\n",
            "Pérdida CLS: 0.0532\n",
            "Train at epoch 4\n",
            "Pérdida CLS: 0.0438\n",
            "Test at epoch 4\n",
            "Pérdida CLS: 0.0436\n",
            "Train at epoch 5\n",
            "Pérdida CLS: 0.0542\n",
            "Test at epoch 5\n",
            "Pérdida CLS: 0.0540\n"
          ]
        }
      ],
      "source": [
        "# increasing the numbers of epochs\n",
        "args.epoch = 50\n",
        "\n",
        "model = train(args)\n",
        "\n",
        "torch.save(model, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Armado del modelo predictivo hídrido\n",
        "\n",
        "Usando el bosque y combinado con el modelo dl_att"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridModel:\n",
        "    def __init__(self, xgb_model_path, dl_model_path):\n",
        "        # Load pre-trained XGBoost model for preprocessing\n",
        "        self.xgb_clf = joblib.load(xgb_model_path)\n",
        "        \n",
        "        # Load pre-trained deep learning model for cross-feature embeddings\n",
        "        self.dl_model = torch.load(dl_model_path)\n",
        "        self.dl_model.eval()\n",
        "        \n",
        "    def process_leaf_idx(self, X_leaves):\n",
        "        \"\"\"\n",
        "        This function is to convert the output of XGBoost model to the input of the deep learning model.\n",
        "        \"\"\"\n",
        "        leaves = X_leaves.copy()\n",
        "        new_leaf_index = dict()  # Dictionary to store leaf index\n",
        "        total_leaves = 0\n",
        "        for c in range(X_leaves.shape[1]):  # Iterate for each column (i.e. 100 trees)\n",
        "            column = X_leaves[:, c]\n",
        "            unique_vals = list(sorted(set(column)))\n",
        "            new_idx = {v: (i + total_leaves) for i, v in enumerate(unique_vals)}\n",
        "            for i, v in enumerate(unique_vals):\n",
        "                leaf_id = i + total_leaves\n",
        "                new_leaf_index[leaf_id] = {c: v}\n",
        "            leaves[:, c] = [new_idx[v] for v in column]\n",
        "            total_leaves += len(unique_vals)\n",
        "\n",
        "        assert leaves.ravel().max() == total_leaves - 1\n",
        "        return leaves, total_leaves, new_leaf_index\n",
        "\n",
        "    def get_dataloader(self, X, batch_size=128):\n",
        "        # Get leaf index from XGBoost model\n",
        "        X_leaves = self.xgb_clf.apply(X)\n",
        "        \n",
        "        # Process leaf indexes\n",
        "        transformed_leaves, _, _ = self.process_leaf_idx(X_leaves)\n",
        "        \n",
        "        # Convert to torch tensor\n",
        "        transformed_leaves = torch.tensor(transformed_leaves).long()\n",
        "        \n",
        "        # Create dataset and dataloader\n",
        "        x_dataset = Data.TensorDataset(transformed_leaves)\n",
        "        x_loader = Data.DataLoader(dataset=x_dataset, batch_size=batch_size, shuffle=False)\n",
        "        \n",
        "        return x_loader\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Get dataloader for input data\n",
        "        dataloader = self.get_dataloader(X)\n",
        "        \n",
        "        # Predict probabilities\n",
        "        probabilities = []\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                batch_input = batch[0]\n",
        "                batch_output = self.dl_model(batch_input)[0]  # Extract tensor from tuple\n",
        "                probs = torch.sigmoid(batch_output).squeeze().cpu().numpy()\n",
        "                probabilities.extend(probs)\n",
        "        \n",
        "        return np.array(probabilities)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        # Predict probabilities\n",
        "        probas = self.predict_proba(X)\n",
        "        \n",
        "        # Convert probabilities to binary predictions based on threshold\n",
        "        predictions = (probas >= threshold).astype(int)\n",
        "\n",
        "        return predictions\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.dl_model.parameters() if p.requires_grad)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prueba del modelo híbrido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\santt\\AppData\\Local\\Temp\\ipykernel_24104\\1537339333.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.dl_model = torch.load(dl_model_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.502487 0.502487 0.502487 ... 0.502487 0.502487 0.502487]\n",
            "[1 1 1 ... 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "xgb_model_path = 'modelos/xgb_classifier_model.pkl'\n",
        "dl_model_path = 'modelos/DMEyF_ensamble1730127357.2834532.pkl'\n",
        "\n",
        "# Instantiate the HybridModel\n",
        "hybrid_model = HybridModel(xgb_model_path, dl_model_path)\n",
        "\n",
        "# junio como mes de test\n",
        "mes_test = 202106\n",
        "\n",
        "X_ = data[data['foto_mes'] == mes_test]\n",
        "\n",
        "target_multi = 'clase_ternaria'\n",
        "X_ = X_.drop(columns=[target_multi])\n",
        "\n",
        "# Predict probabilities\n",
        "probas = hybrid_model.predict_proba(X_)\n",
        "print(probas)\n",
        "\n",
        "# Predict classes\n",
        "predictions = hybrid_model.predict(X_)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1])"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "71505"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hybrid_model.count_parameters()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dm_eyf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
